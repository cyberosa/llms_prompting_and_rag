{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which type of fine tuning?\n",
    "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve the model performance and generalization to an unseen task. There are several types of fine-tuning that we can perform:\n",
    "\n",
    "* single-task fine-tuning: we just need to re-train the whole model on a new unseen task by providing new data (often just 500-1,000 examples can result in good performance). This process though may lead to catastrophic forgetting. Catastrophic forgetting happens because the full fine-tuning process modifies the weights of the original LLM. While this leads to great performance on the single fine-tuning task, it can degrade performance on other tasks. \n",
    "\n",
    "* multitask fine-tuning: good multitask fine-tuning may though require 50-100,000 examples across many tasks, and so will require more data and more computation.\n",
    "\n",
    "* parameter efficient fine-tuning (PEFT): PEFT is a set of techniques that preserves the weights of the original LLM and trains only a small number of task-specific adapter layers and parameters. PEFT shows greater robustness to catastrophic forgetting since most of the pre-trained weights are left unchanged\n",
    "\n",
    "There are many factors to take into account to choose one option:\n",
    "1. size of the model\n",
    "2. hardware available\n",
    "3. purpose of the model\n",
    "\n",
    "For the use-case we have here to teach a model on giving advice for bets, maybe we could afford the single-task fine-tuning option and loose the multitask generalized capabilities. Besides we took a small model of 7B size.\n",
    "\n",
    "However if we decide to go for a bigger model then full fine-tuning might be computationally expensive in terms of hardware so PEFT is the best option here as it can often be performed on a single GPU. The original LLM is only slightly modified or left unchanged, so PEFT is less prone to the catastrophic forgetting problems of full fine-tuning. There are several PEFT techniques and we would chose the most common one called *Low-rank Adaptation*, or LoRA for short. It is a parameter-efficient fine-tuning technique that falls into the re-parameterization category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in fine-tuning\n",
    "\n",
    "### Load your model and needed libraries\n",
    "In this exercise mistral and llama2 were proposed but we found for instance this already pre-trained model with plenty of sports articles\n",
    "\n",
    "https://huggingface.co/microsoft/SportsBERT\n",
    "\n",
    "*portsBERT is a BERT model trained from scratch with specific focus on sports articles. The training corpus included news articles scraped from the web related to sports from the past 4 years. These articles covered news from Football, Basketball, Hockey, Cricket, Soccer, Baseball, Olympics, Tennis, Golf, MMA, etc. There were approximately 8 million training samples*\n",
    "\n",
    "So we could use this one as the base model to create a final one trained to predict outcomes.\n",
    "\n",
    "A model needs some libraries to prepare tokens, configuration, etc. For instance the transformers library provides some.\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "```\n",
    "\n",
    "#### For full fine-tuning a 7B model\n",
    "```\n",
    "from transformers import AutoModelForCausalLM, \n",
    "```\n",
    " For text generation tasks, you would typically use a model from the AutoModelForCausalLM family. These models are specifically designed for autoregressive language modeling tasks, which include text generation.\n",
    "#### For LoRA fine-tuning of bigger models\n",
    "You need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter.\n",
    "```\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "```\n",
    "\n",
    "\n",
    "### Find a dataset\n",
    "\n",
    "In huggingface there are many different datasets of a myriad of categories. For instance this dataset contains the results of many basketball matches so it could be used to fine-tune a model to predict the outcomes of basketball games\n",
    "https://huggingface.co/datasets/GEM/sportsett_basketball\n",
    "\n",
    "\n",
    "### Data preparation\n",
    "The dataset needs to have the right labels, in our case would be 'question', 'outcome', 'probability' and 'confidence_interval'. Furthermore we need to split this dataset into train, test and validation. LetÂ´s suppose we have around 1500 samples so a would distribution of the three groups could be:\n",
    "```\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'question', 'outcome', 'probability', 'confidence_interval'],\n",
    "        num_rows: 1246\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'question', 'outcome', 'probability', 'confidence_interval'],\n",
    "        num_rows: 150\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['id', 'question', 'outcome', 'probability', 'confidence_interval'],\n",
    "        num_rows: 50\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model trainable parameters\n",
    "It is relevant to find out the number of model parameters and how many of them are trainable. There are several functions available to achieve that:\n",
    "\n",
    "```\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataset\n",
    "You need to convert the question-answers (prompt-response) tuples into explicit instructions for the LLM\n",
    "Then preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token). The tokenize function will create the prompt for each sample.\n",
    "\n",
    "Training prompt (question):\n",
    "\"\"\"\n",
    "Answer the following question by giving as output a json response with two fields: \n",
    "the probability of the answer and the confidence interval of the answer. The question is: {question}.\n",
    "\n",
    "Prediction: \n",
    "\"\"\"\n",
    "\n",
    "Training response (prediction):\n",
    "\"\"\"\n",
    "{outcome}, {probability}, {confidence_interval}\n",
    "\"\"\"\n",
    "\n",
    "The output dataset is ready for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We can use the *Trainer* from Hugging Face (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)) and choose different parameters for the finetuning steps.\n",
    "```\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=selected_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "```\n",
    "Finally you can create an instance of the `AutoModelForCausalLM` class for the instruct model using the from_pretrained function.\n",
    "\n",
    "#### If we are using PEFT\n",
    "You need to define the LoRA configuration and create the peft model that will be passed to the Trainer. Some explanations about the different parameters can be found [here](https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578).\n",
    "```\n",
    "peft_config = LoraConfig(\n",
    "      lora_alpha=16,\n",
    "      lora_dropout=0.1,\n",
    "      r=32, # Rank\n",
    "      bias=\"none\",\n",
    "      task_type=\"CAUSAL_LM\")\n",
    "      \n",
    "peft_model = get_peft_model(original_model, \n",
    "                            peft_config)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "You can start using the instruct model and save it in your cloud provider or if you are registered to the huggingface-hub you can push it there.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "We need some metrics to establish evaluation criteria to assess the effectiveness of the prompt set and its impact on the LLM's performance. Common metrics include response accuracy, fluency, coherence, relevance, and completeness.\n",
    "\n",
    "### Evaluate the model qualitatively (Human evaluation)\n",
    "It is always good to manually check first that the model is behaving correctly. Is the output format correct? Do the answers make any sense? If the model is providing external sources or giving references, could we double check that those links and those resources really exist? We could in this category complete some metrics from the outputs such as:\n",
    "* completeness\n",
    "* coherence\n",
    "* reliability\n",
    "\n",
    "\n",
    "### Evaluate the Model Quantitatively\n",
    "For this use-case that we are simulating where we are fine-tuning a model to improve the prediction skills, we can use accuracy to check the number of correct answers the model is guessing. However we are considering also the probability and confidence interval of those right answers, so we could collect all the probabilities of those right answers and check the probability distribution. Are most of my right predictions based on a high probability or the model? is the model confident enough or the confidence intervals are very wide? Considering this we could elaborate some metrics such as:\n",
    "* accuracy\n",
    "* probability distribution of correct answers\n",
    "* probability distribution of wrong answers\n",
    "* size of confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "https://www.coursera.org/learn/generative-ai-with-llms\n",
    "\n",
    "https://www.e2enetworks.com/blog/a-step-by-step-guide-to-fine-tuning-the-mistral-7b-llm\n",
    "\n",
    "LoRA\n",
    "\n",
    "https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "\n",
    "https://arxiv.org/pdf/2106.09685.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
