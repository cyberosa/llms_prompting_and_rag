{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt optimization\n",
    "\n",
    "### Prompt engineering techniques\n",
    "Optimizing prompt engineering with large language models (LLMs) involves refining the input prompts to achieve better performance or generate desired outputs. Some common strategies for optimizing prompt engineering are:\n",
    "\n",
    "1. **Experimentation and Iteration:**\n",
    "   - Start with a simple prompt and iteratively refine it based on model outputs.\n",
    "   - Experiment with different phrasings, structures, and lengths to find what works best.\n",
    "\n",
    "2. **Prompt Tailoring:**\n",
    "   - Tailor prompts to the specific capabilities and limitations of the model.\n",
    "   - Adjust prompts to guide the model towards the desired behavior.\n",
    "\n",
    "3. **Temperature and Top-k Sampling:**\n",
    "   - Adjust temperature and top-k sampling parameters to control the randomness and diversity of generated outputs.\n",
    "   - Lower temperatures (e.g., 0.5) produce more deterministic outputs, while higher values (e.g., 1.0) introduce more randomness.\n",
    "\n",
    "4. **Prompt Engineering Libraries:**\n",
    "   - Leverage prompt engineering libraries designed for specific models, such as OpenAI's prompt-engineering for GPT-3.\n",
    "   - These libraries often provide pre-built functions and templates for effective prompt engineering.\n",
    "\n",
    "5. **Domain-Specific Knowledge:**\n",
    "   - Incorporate domain-specific knowledge into prompts to enhance model performance on specific tasks.\n",
    "   - Guide the model with task-specific instructions or context.\n",
    "\n",
    "6. **Chain of thought prompting:**\n",
    "   - Combine multiple prompts or input sequences to provide a broader context or set of instructions.\n",
    "   - Concatenate relevant information to help the model understand the desired task more comprehensively.\n",
    "\n",
    "7. **Fine-Tuning:**\n",
    "    - Fine-tune the model on specific tasks or domains to improve performance.\n",
    "    - This requires access to labeled data for the specific task.\n",
    "\n",
    "8. **Bias Mitigation:**\n",
    "    - Carefully design prompts to mitigate biases in model outputs.\n",
    "    - Consider using debiasing techniques or introducing counterfactuals to address potential biases.\n",
    "\n",
    "9. **Prompt Evaluation Metrics:**\n",
    "    - Establish clear evaluation metrics for generated outputs.\n",
    "    - Use these metrics to quantify the model's performance and guide prompt refinement.\n",
    "\n",
    "10. **Prompt Setting:** \n",
    "    - Combine a structured set of tokens, that were used on the LLM during training, that elicit desirable responses from an LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization using **Retrieval Augmented Generation (RAG)**\n",
    "When the problem of not getting good respones is a lack of specific relevant information (context) on the training data of the LLM, then a more efficient technique is using RAG. The RAG technique helps LLMs by giving them access to external data so that they can generate a response with additional context.\n",
    "\n",
    "In this exercise we were asked to predict the outcome of specific sport events for instance and the model itself was suggesting us to use this approach to have more reliable and accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"../images/image.png\" width=\"550\"/>](attachment:../images/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve the results of this exercise?\n",
    "We are evaluating the prediction skills of these models for two specific domains: Sport events outcomes and science questions (including some weather forecasting ones). The first prompt we tried included already instructions about the specific format of the desired output. We succeded in the way that the two models answered with the correct format but soon we realized that most of the answers are purely random.\n",
    "\n",
    "Models can sometimes make up events and facts that do not allign with reality because they are optimized to imitate human reasoning and not to say always the truth. Thus we cannot use the metric \"prediction accuracy\" here to guide our prompt engineering strategy. Using the same words as Andrew Ng:\n",
    "\n",
    "\"Sometimes people use LLMs as a repository of facts to get answers to their questions. But I think there's a different and maybe more helpful way to think of them, which as reasoning engines. You can give it APIs to go get its own facts\"\n",
    "\n",
    "Read the article attached in the Resources chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to lack of time to try more strategies we are going to test only two: RAG and chain of thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG)\n",
    "We will use this technique to improve the questions regarding sport events and we will take one question as an example to check how the model improves the answer.\n",
    "The idea is to combine domain expertise and other forecasting methods to enhance the overall accuracy and reliability of the LLM predictions.\n",
    "We are going to follow the same POC as the one referenced in resources. \n",
    "\n",
    "See results at the notebook Rag_optimization.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Setting\n",
    " By carefully crafting the input prompts, you can steer the model toward generating more relevant, coherent, and informative answers to specific questions or tasks. Effective prompt setting can enhance the overall quality and versatility of LLM responses, ultimately leading to improved decision-making, problem-solving, or communication capabilities\n",
    "\n",
    "This option is only valid for open source LLMs, which has this hidden configuration to improve the answers provided by the model. We found it thanks to the Deeplearning.AI community where I am a member and contributor.\n",
    "\n",
    "\n",
    "### Mistral model\n",
    "The format of the prompt needs to be:\n",
    "```\n",
    "<s>[INST] {text} [/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring the probabilities, this was the answer of the model to the question:\n",
    "\n",
    "*Iceland volcanic eruption by December 31st 2023?*\n",
    "\n",
    "Answer with no tags:\n",
    "\n",
    "> I cannot predict the future or provide accurate information about whether a volcanic eruption will occur in Iceland by December 31, 2023. It is important to rely on trusted sources like the Icelandic Met Office for real-time updates and reliable information regarding volcanic activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s try the tags and compare responses:\n",
    "\n",
    "Answer with tags:\n",
    "\n",
    "> Predicting specific natural disasters like volcanic eruptions is not something that can be accurately determined based on current data or my capabilities as an AI language model. While Iceland is home to many active volcanoes, there is no credible evidence to suggest that any will erupt by December 31, 2023. Monitoring agencies such as the Icelandic Met Office provide up-to-date information on volcanic activity and potential threats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the second answer is much better than the first one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take another question where we could evaluate even a better improvement in the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*provide a template for a smart contract on the Ethereum blockchain*\n",
    "\n",
    "> Creating a complete and functional template for a smart contract on the Ethereum blockchain goes beyond the scope of this conversation. However, I can provide you with a basic outline and structure for a simple ERC-20 token smart contract using Solidity programming language. Please note that this is for educational purposes only and should not be used as a basis for creating a production smart contract without proper testing and security review.\n",
    "```\n",
    "pragma solidity ^0.8.0;\n",
    "\n",
    "import \"@openzeppelin/contracts/token/ERC20/ERC20.sol\";\n",
    "import \"@openzeppelin/contracts/access/Ownable.sol\";\n",
    "\n",
    "contract MyToken is ERC20, Ownable {\n",
    "   uint256 private constant _name = \"My Token\";\n",
    "   uint256 private constant _symbol = \"MTK\";\n",
    "   uint256 private constant _initialSupply = 1000000;\n",
    "\n",
    "   constructor() ERC20(\"My Token\", \"MTK\") public {\n",
    "       _mint(msg.sender, _initialSupply);\n",
    "   }\n",
    "}\n",
    "```\n",
    "> This contract creates an ERC-20 compatible token called MyToken, with a name of \"My Token\" and a symbol of \"MTK\". The initial supply of tokens is set to 1,000,000, and the contract is owned by the deployer address. This is just a starting point, and you would need to add more functionality, such as transfer, approve, and other features, depending on your use case. For more complex smart contracts, consider consulting experienced developers and following industry best practices for development and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer with tags:\n",
    "\n",
    ">Here's a simple example of a smart contract template written in Solidity for an ERC-20 compliant token on the Ethereum blockchain. Keep in mind that this is just a basic template, and it might not include all necessary functions or features for a production-ready contract. You may need to consult experienced developers or expand upon this template for more advanced functionality.\n",
    "```\n",
    "// SPDX-License-Identifier: MIT\n",
    "pragma solidity ^0.8.0;\n",
    "\n",
    "import \"@openzeppelin/contracts/token/ERC20/ERC20.sol\";\n",
    "import \"@openzeppelin/contracts/security/Pausable.sol\";\n",
    "import \"@openzeppelin/contracts/access/Ownable.sol\";\n",
    "\n",
    "contract MyToken is ERC20, Pausable, Ownable {\n",
    "   string public name = \"MyToken\";\n",
    "   string public symbol = \"MTK\";\n",
    "   uint8 public decimals = 18;\n",
    "   uint256 public totalSupply;\n",
    "\n",
    "   constructor() ERC20(\"MyToken\", \"MTK\") {}\n",
    "\n",
    "   function mint(address to, uint amount) external onlyOwner {\n",
    "       require(_mintabilityChecks(msg.sender, amount), \"MINTING_NOT_ALLOWED\");\n",
    "       _safeMint(to, amount);\n",
    "       emit Transfer(address(0), to, amount * (10 ** uint256(decimals));\n",
    "   }\n",
    "\n",
    "   function burn(uint amount) external payable {\n",
    "       require(balanceOf(msg.sender) >= amount, \"Insufficient balance\");\n",
    "       _burn(msg.sender, amount);\n",
    "       emit Transfer(msg.sender, address(0), amount * (10 ** uint256(decimals)));\n",
    "   }\n",
    "\n",
    "   function pause() external onlyOwner {\n",
    "       _pause();\n",
    "   }\n",
    "\n",
    "   function unpause() external onlyOwner {\n",
    "       _unpause();\n",
    "   }\n",
    "}\n",
    "```\n",
    "> This contract is a simple ERC-20 token named MyToken with the symbol MTK. It includes the Pausable and Ownable OpenZeppelin modules for managing pausing and ownership, respectively. The mint() and burn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2\n",
    "Previously we did not use the system prompt but we are going to use it now.\n",
    "For this model the improvement of the anwer is achieved using this format:\n",
    "```\n",
    "<s> [INST] <<SYS>>\\n{system_text}<</SYS>>\\n{text} [/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat one of the questions we formulated before:\n",
    "*Is the Room-Temperature Superconductor discovered in 2023 real?*\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\\nyou are responding to highly technical people\\n<</SYS>>\\n{question} [/INST]\n",
    "```\n",
    "We get a more detailed response:\n",
    "\n",
    "> Ah, an interesting question! adjusts glasses\n",
    "\n",
    "> As a Large Language Model, I must inform you that the claim of a Room-Temperature Superconductor discovered in 2023 is indeed a topic of ongoing research and debate in the scientific community. While there have been numerous reports and studies published on the subject, the veracity of these claims remains a matter of investigation and validation through rigorous experimentation and peer review.\n",
    "\n",
    "> Superconductivity is a phenomenon where certain materials exhibit zero electrical resistance when cooled below a certain temperature, known as the critical temperature (Tc). Achieving superconductivity at room temperature (i.e., above Tc) would revolutionize many industries, including energy transmission, medical imaging, and high-energy physics.\n",
    "\n",
    "> In recent years, several groups have reported observations of superconductivity at temperatures near or above room temperature under high pressure, magnetic field application, or exposure to certain chemical compounds. However, these findings are often controversial, and the observed effects may not necessarily arise from traditional phonons (lattice vibrations), which are the primary mediators of superconductivity.\n",
    "\n",
    "> To establish the existence of a room-temperature superconductor, extensive experimental and theoretical investigations are required to rule out alternative explanations, replicate results consistently, and validate the mechanisms involved. For example, advanced characterization techniques such as scanning tunneling microscopy, angle-resolved photoemission spectroscopy, and neutron scattering experiments can help elucidate the underlying physics.\n",
    "\n",
    "> While significant progress has been made toward this goal, a fully realized room-temperature superconductor remains an open challenge in condensed matter physics. As a field, we are eagerly awaiting further breakthroughs and discoveries that could lead to practical applications and unlock new possibilities for technological advancements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s try another question using the default system prompt:\n",
    "\n",
    "*\"Is it going to rain tomorrow in London?\"*\n",
    "\n",
    "> I am unable to predict the weather with certainty as I do not have access to real-time weather data or the ability to observe current weather conditions. Additionally, weather forecasting is a complex task that involves analyzing a wide range of factors, including atmospheric pressure, humidity, wind patterns, and more.\n",
    "> However, I can suggest some resources that might be helpful in determining whether it will rain in London tomorrow. You can check the website of the Met Office, the UK's national meteorological service, which provides detailed weather forecasts and warnings. Alternatively, you can use a weather app or website that uses machine learning algorithms to provide accurate and personalized weather predictions based on historical data and real-time conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adding a specific system prompt:\n",
    "\n",
    "{\"system\": \"you are a weather forecasting expert\", \"user\": \"Is it going to rain tomorrow in London?\"}\n",
    "\n",
    "> Ah, an intriguing query! adjusts sunglasses\n",
    "> As a seasoned weather forecaster, I must inform you that predicting the exact precipitation pattern in a specific location like London over a short time frame like tomorrow is inherently challenging due to various factors affecting the local weather system. However, I can provide some insights based on the latest weather models and trends.\n",
    "> Currently, the European Centre for Medium-Range Weather Forecasts (ECMWF) model indicates a low-pressure system located over western Europe, which is expected to move eastward and bring scattered showers to the UK tomorrow. The GFS model, on the other hand, suggests a higher chance of precipitation in southern England, particularly in the afternoon hours.\n",
    "> However, please keep in mind that both of these models are only predictions and are subject to change as new data becomes available. In addition, local weather phenomena, such as the proximity to bodies of water or mountainous regions, can significantly impact the actual precipitation amount and timing.\n",
    "> If you want a more accurate prediction, you can consult the Met Office or other reliable weather sources, which often provide more detailed and up-to-date information based on their exclusive datasets and advanced forecasting techniques. And remember, always stay prepared for any weather condition by checking the latest forecasts before heading outside!\n",
    "\n",
    "We improved the answer though we are not 100% if those referenced forecasting reports are actual or just made up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally letÂ´s try with the tags:\n",
    "\n",
    "[<img src=\"../images/llama2_best_answer.png\" width=\"750\" />]\n",
    "\n",
    "\n",
    "Indeed this is the best answer so far and interestingly it included emojis to the answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and takeaways\n",
    "We covered a couple of techniques to improve the answers of our models:\n",
    "\n",
    "* RAG\n",
    "* Prompt setting \n",
    "\n",
    "Considering that the goal is to improve the prediction skills of these models then the RAG option is definitely the way to go, providing specific context-related articles where the model can find relevant information to answer the questions with more accuracy. We probed that using one of the failed questions and the result was amazingly correct and very well explained.\n",
    "We could even combine both strategies by introducing the effective prompt setting with RAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "\n",
    "LLMs as reasoning engines\n",
    "https://www.nature.com/articles/s41591-023-02594-z\n",
    "\n",
    "Langchain\n",
    "https://python.langchain.com/docs/get_started/introduction\n",
    "\n",
    "Prompting with Llama2\n",
    "https://llama-2.ai/prompting-llama-2/\n",
    "\n",
    "RAG\n",
    "https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146\n",
    "\n",
    "Prompt engineering in open-source LLMs\n",
    "https://github.com/lamini-ai/prompt-engineering-open-llms/tree/master\n",
    "\n",
    "https://www.youtube.com/watch?v=f32dc5M2Mn0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
