{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between the two models\n",
    "According to research and published articles on common LLMs' metrics, Mistral 7B significantly outperforms Llama 2 13B on all metrics:\n",
    "\n",
    "\n",
    "https://mistral.ai/news/announcing-mistral-7b/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset with responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>year</th>\n",
       "      <th>mistral_responses_iter1</th>\n",
       "      <th>llama2_responses_iter1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the cricket IPL game, scheduled for May 14t...</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>sport</td>\n",
       "      <td>2023</td>\n",
       "      <td>I'm an AI, I don't have the ability to predict...</td>\n",
       "      <td>{\\n\"probs\": 0.7,\\n\"confidence_interval\": [0.6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the NBA basketball game, scheduled for Octo...</td>\n",
       "      <td>Pistons</td>\n",
       "      <td>sport</td>\n",
       "      <td>2023</td>\n",
       "      <td>{\\n  \"probability\": 0.45,\\n  \"confidence_inter...</td>\n",
       "      <td>{\\n\"probability\": 0.7,\\n\"confidenceInterval\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the cricket IPL game, scheduled for May 14t...</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>sport</td>\n",
       "      <td>2023</td>\n",
       "      <td>{\\n  \"probability\": 0.51,\\n  \"confidence_inter...</td>\n",
       "      <td>{\\n\"probability\": 0.7,\\n\"confidence_interval\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the NBA basketball game, scheduled for Octo...</td>\n",
       "      <td>Pistons</td>\n",
       "      <td>sport</td>\n",
       "      <td>2023</td>\n",
       "      <td>{\\n  \"probability\": 0.5,\\n  \"confidence_interv...</td>\n",
       "      <td>{\\n\"probability\": 0.75,\\n\"confidence\": 0.9\\n}\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Will Netherlands qualify for the World Cup in ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>sport</td>\n",
       "      <td>2023</td>\n",
       "      <td>{\\n\"probability\": 0.8,\\n\"confidence_interval\":...</td>\n",
       "      <td>{\\n\"proba\": 0.75,\\n\"conf\": [0.6, 0.9]\\n}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question     answer category  \\\n",
       "0  In the cricket IPL game, scheduled for May 14t...  Bangalore    sport   \n",
       "1  In the NBA basketball game, scheduled for Octo...    Pistons    sport   \n",
       "2  In the cricket IPL game, scheduled for May 14t...  Bangalore    sport   \n",
       "3  In the NBA basketball game, scheduled for Octo...    Pistons    sport   \n",
       "4  Will Netherlands qualify for the World Cup in ...        Yes    sport   \n",
       "\n",
       "   year                            mistral_responses_iter1  \\\n",
       "0  2023  I'm an AI, I don't have the ability to predict...   \n",
       "1  2023  {\\n  \"probability\": 0.45,\\n  \"confidence_inter...   \n",
       "2  2023  {\\n  \"probability\": 0.51,\\n  \"confidence_inter...   \n",
       "3  2023  {\\n  \"probability\": 0.5,\\n  \"confidence_interv...   \n",
       "4  2023  {\\n\"probability\": 0.8,\\n\"confidence_interval\":...   \n",
       "\n",
       "                              llama2_responses_iter1  \n",
       "0  {\\n\"probs\": 0.7,\\n\"confidence_interval\": [0.6,...  \n",
       "1  {\\n\"probability\": 0.7,\\n\"confidenceInterval\": ...  \n",
       "2  {\\n\"probability\": 0.7,\\n\"confidence_interval\":...  \n",
       "3    {\\n\"probability\": 0.75,\\n\"confidence\": 0.9\\n}\\n  \n",
       "4           {\\n\"proba\": 0.75,\\n\"conf\": [0.6, 0.9]\\n}  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "data_path = os.path.join(parent_dir, \"data\")\n",
    "responses_df = pd.read_pickle(os.path.join(data_path,\"responses_df\"))\n",
    "responses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the first two questions were repeated so we delete them. Interestingly, the models gave a different answer for the very same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df.drop([2,3], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to collect the correct answers\n",
    "mistral_correct_responses = []\n",
    "llama2_correct_responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the cricket IPL game, scheduled for May 14th in 2023, which team is going to win: Rajasthan or Bangalore?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[0].question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral response\n",
    "It gives a very long answer (note: to improve prompt engineering adding a max nr of output tokens parameter) and it is giving the higher probability to Rajasthan though both won the same amount of matches. So it did not guess the correct answer though this model might indeed have even the results of this past match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #31708f; background-color: #d9edf7; border-color: #bce8f1;\">\n",
    "'I\\'m an AI, I don\\'t have the ability to predict or know the outcome of a cricket match before it happens. However, I can give you some general information or historical data that might help you make an informed decision. For instance, if we look at the most recent performance and head-to-head records, we find that:\\n\\n1. Rajasthan won 5 of the last 10 matches against Bangalore.\\n2. Bangalore won 5 of the last 10 matches against Rajasthan.\\n\\nSo, based on this data, we could estimate that the probability of Rajasthan winning is around 0.52 (52%) and the probability of Bangalore winning is around 0.48 (48%). However, these probabilities only reflect historic performance and do not account for any potential injuries, team composition changes, or unexpected factors that could influence the outcome of the match.\\n\\nAs for the confidence interval, it\\'s important to keep in mind that predicting sports outcomes is inherently uncertain. Based on the binary nature of the question ( Rajasthan or Bangalore), a confidence interval can\\'t be directly calculated using a standard normal distribution since the data is not continuous. Nevertheless, we could represent the uncertainty using a range or interval based on the probabilities and some assumption of potential swing in team performance. For instance, considering the close probability values, a reasonable confidence interval could be something like [0.45, 0.59] for Rajasthan\\'s win percentage or [0.41, 0.54] for Bangalore\\'s win percentage. Please note that I cannot guarantee the validity of these confidence intervals, as cricket match outcomes include a significant element of randomness and uncertainty.\\n\\nThe output for the JSON response could look like:\\n{\\n  \"probability_Rajasthan\": 0.52,\\n  \"confidence_interval_Rajasthan\": [0.45, 0.59]\\n},\\n{\\n  \"probability_Bangalore\": 0.48,\\n  \"confidence_interval_Bangalore\": [0.41, 0.54]\\n}\\n\\nIt\\'s important to keep in mind that historical data is not always a perfect indicator of future performance and there are many factors that can influence the outcome of a single match. Careful consideration of all relevant information and expertise is required when making predictions.'</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_correct_responses.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 response\n",
    "This model is giving us a short answer directly with the probability but no explanation and no clear answer so it maybe did not understand the instruction. This model is older than mistral and less advanced, so probably its reasoning engine is not as intelligent as mistral's one. Thus the same prompt does not work for this type of question. We consider it as a failed answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"probs\": 0.7,\\n\"confidence_interval\": [0.6, 0.85]\\n}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[0].llama2_responses_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama2_correct_responses.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the NBA basketball game, scheduled for October 27th in 2023, which team is going to win: Pistons or Hornets?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[1].question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral response\n",
    "This time it did not answer the question and just gave a probability number. In fact the same question was asked twice, due to some error with the request sending, and the second time it answered with a different probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"probability\": 0.45,\\n  \"confidence_interval\": [0.4, 0.5]\\n}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[1].mistral_responses_iter1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 response\n",
    "Again it did not answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"probability\": 0.7,\\n\"confidenceInterval\": [0.5,0.9]\\n}\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[1].llama2_responses_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_correct_responses.append(0)\n",
    "llama2_correct_responses.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Will Netherlands qualify for the World Cup in 2023?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[2].question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral response\n",
    "Again it is not clear if it understood the question, but this time is a yes/no answer so we could take this probability as the probability of Netherlands to qualify. In that case is a yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"probability\": 0.8,\\n\"confidence_interval\": [0.7, 0.9]\\n}\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[2].mistral_responses_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_correct_responses.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 response\n",
    "Same as for the mistral response, we will take it as a yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"proba\": 0.75,\\n\"conf\": [0.6, 0.9]\\n}'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_df.iloc[2].llama2_responses_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_correct_responses.append(1)\n",
    "llama2_correct_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions  4, 5, 6 and 7\n",
    "They all refer to questions about some specific country qualifying for the World Cup in 2023\n",
    "\n",
    "Will Senegal qualify for the World Cup in 2023?\n",
    "Will France qualify for the World Cup in 2023?\n",
    "Will Spain qualify for the World Cup in 2023?\n",
    "Will Qatar qualify for the World Cup in 2023?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral responses\n",
    "It says yes for all, being Qatar the one with the highest and it gives the rationale of being the host country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_correct_responses.append(1)\n",
    "mistral_correct_responses.append(1)\n",
    "mistral_correct_responses.append(1)\n",
    "mistral_correct_responses.append(0)\n",
    "mistral_correct_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 responses\n",
    "This model is also saying yes for all but with lower probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_correct_responses.append(1)\n",
    "llama2_correct_responses.append(1)\n",
    "llama2_correct_responses.append(1)\n",
    "llama2_correct_responses.append(0)\n",
    "llama2_correct_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions from 8 to 17\n",
    "Will Bryce Young be first QB selected in the 2023 NFL draft?\n",
    "\n",
    "Will Tadej Pogačar win the 2023 Tour de France?\n",
    "\n",
    "In the 2023 Qatar Open Final in Tennis, who is going to win Swiatek or Pegula?\n",
    "\n",
    "Will Coco Gauff win Women's US Open 2023 in Tennis?\n",
    "\n",
    "On December the 19th of 2022 who is going to win the NFL Rams or Packers\n",
    "\n",
    "In the cricket IPL game, scheduled for April the 1st in 2023, which team is going to win: Punjab or Kolkata\n",
    "\n",
    "Will Hans Niemann finish top 3 in the U.S. Chess Championship in 2022?\n",
    "\n",
    "Will Magnus Carlsen win the Tata Steel Masters 2023?\n",
    "\n",
    "Within the Super Bowl LVII in February 2023, which team is going to winEagles or Chiefs\n",
    "\n",
    "In the World Cup Final, scheduled for December 18th in 2022, which country is going to win France or Argentina?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all sports questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral responses\n",
    "For most of the questions is giving almost 50% probability with the answer, which means it really has no idea. For instance the answer to question 8 it is the same as answering yes and no.\n",
    "\n",
    "'{\\n\"probability\": 0.55,\\n\"confidence_interval\": [0.45, 0.65]\\n}\n",
    "Bryce Young has a 55% chance of being the first quarterback selected in the NFL draft. However, the confidence interval is quite wide, ranging from 45% to 65%, due to the uncertainty of team needs, player performance, and other factors that can influence draft decisions.'\n",
    "\n",
    "Only for a couple of questions the model showed a strong No or a strong Yes and it did not guess it right.\n",
    "\n",
    "Adding the responses to the vector collecting the right answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_correct_responses.append(1) # 8\n",
    "mistral_correct_responses.append(0) # 9\n",
    "mistral_correct_responses.append(1) # 10\n",
    "mistral_correct_responses.append(0) # 11\n",
    "mistral_correct_responses.append(0) # 12 \n",
    "mistral_correct_responses.append(1) # 13\n",
    "mistral_correct_responses.append(1) # 14\n",
    "mistral_correct_responses.append(0) # 15\n",
    "mistral_correct_responses.append(0) # 16\n",
    "mistral_correct_responses.append(0) # 17\n",
    "mistral_correct_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral model is more advance and within some of the answers is giving us already comments that what we are trying to achieve with the model is a wrong use-case and these LLMs models are reasoning engines and not based-on-facts database that could be used to get accurate predictions.\n",
    "\n",
    "Some of the warnings from the answers:\n",
    "\n",
    "<div style=\"padding: 15px; border: 1px solid transparent; border-color: transparent; margin-bottom: 20px; border-radius: 4px; color: #8a6d3b;; background-color: #fcf8e3; border-color: #faebcc;\">\n",
    "'Please note that this is a simulated answer and should not be considered as an accurate prediction.'\n",
    "\n",
    "' Real-life predictions would require access to real-time data and up-to-date injury reports.'\n",
    "\n",
    "' For the most accurate information, refer to reliable sports news sources or betting odds just before the match.\\n\\nThis is the response you\\'d get on average, but the model might change depending on the latest news, injuries or other unpredicted factors. So every time you ask this question, the probability might change.\\n\\nNote: I don\\'t have access to real-time data, I\\'m just a Large Language Model, my goal is to mimic human-like conversation and generate outputs that seem plausible given the context.'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 responses\n",
    "The model is just answering with a specific probability number and no explanations. The probabilities are slightly higher that those from mistral model. Besides they are almost all yes to the first option of the two, showing a similar trend as for Mistral when giving two choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_correct_responses.append(1) # 8\n",
    "llama2_correct_responses.append(0) # 9\n",
    "llama2_correct_responses.append(1) # 10\n",
    "llama2_correct_responses.append(1) # 11\n",
    "llama2_correct_responses.append(0) # 12 \n",
    "llama2_correct_responses.append(1) # 13\n",
    "llama2_correct_responses.append(1) # 14\n",
    "llama2_correct_responses.append(0) # 15\n",
    "llama2_correct_responses.append(0) # 16\n",
    "llama2_correct_responses.append(0) # 17\n",
    "llama2_correct_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 18-32: Science questions\n",
    "Most of the found questions are related with weather disasters and hypothetical pseudo-scientific discoveries.\n",
    "\n",
    "Will Idalia hit Florida as a major hurricane in August 2023?\n",
    "\n",
    "Will a hurricane make landfall in the US in November 2023?\n",
    "\n",
    "Will Lee become a Category 5 hurricane in September 2023?\n",
    "\n",
    "Will Artemis or Starship reach space first in 2023?\n",
    "\n",
    "Will SpaceX's Starship successfully reach outer space by March 31, 2023?\n",
    "\n",
    "Are the Peruvian Aliens real?\n",
    "\n",
    "Will NASA record 2023 as the hottest year on record (1.03°C or higher)?\n",
    "\n",
    "Will there be widespread flooding in LA in August 2023?\n",
    "\n",
    "Iceland volcanic eruption by December 31st 2023?\n",
    "\n",
    "Will Hilary hit US as a hurricane in August 2023?\n",
    "\n",
    "Alien contact in 2023?\n",
    "\n",
    "Will SpaceX's Starship lift 500m from pad by November 19th 2023?\n",
    "\n",
    "Is the Room-Temperature Superconductor discovered in 2023 real?\n",
    "\n",
    "Iceland volcanic eruption by November 17th 2023?\n",
    "\n",
    "Is the Earth flat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral responses\n",
    "For this category we can identify more negative answers. Except for the pseudo-science questions, where we can identify a strong confidence, the rest of answers are closer to a 0.5 probability. This is close to say \"I do not know so yes and no\" . One example: {\\n\"probability\": 0.45,\\n\"confidence_interval\": [0.3, 0.6]\\n}\n",
    "\n",
    "However it was able to retrieve some related information linked to the questions, though there is no 100% guarantee they are true.\n",
    "\n",
    "It is giving the advice to reach proper sources of reliable information:\n",
    "\n",
    "\"Keep in mind that this response is a rough estimate, based on general knowledge and historical data. Actual conditions could differ materially. I strongly recommend consulting meteorological and disaster response agencies for the most accurate and up-to-date information\"\n",
    "\"The best source for up-to-date information on storm track and intensity would be NOAA\\'s National Hurricane Center\"\n",
    "\n",
    "\n",
    "\n",
    "And it is giving us a very clear warning:\n",
    "\"I must first clarify that I don't have real-time or predictive capabilities. I'm an AI language model, not a weather or disaster prediction service.\"\n",
    "\"I strongly recommend consulting meteorological and disaster response agencies for the most accurate and up-to-date information.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_correct_responses.append(0) # 18 Florida\n",
    "mistral_correct_responses.append(1) # 19 landfall\n",
    "mistral_correct_responses.append(0) # 20 Lee\n",
    "mistral_correct_responses.append(0) # 21 Artemis\n",
    "mistral_correct_responses.append(0) # 22 Space X\n",
    "mistral_correct_responses.append(1) # 23 Peruvian aliens\n",
    "mistral_correct_responses.append(0) # 24 hottest year\n",
    "mistral_correct_responses.append(0) # 25 flooding in LA\n",
    "mistral_correct_responses.append(0) # 26 eruption in Iceland in December\n",
    "mistral_correct_responses.append(1) # 27 Hilary hit\n",
    "mistral_correct_responses.append(1) # 28 Aliens\n",
    "mistral_correct_responses.append(0) # 29 SpaceX lift 500m\n",
    "mistral_correct_responses.append(1) # 30 Superconductor\n",
    "mistral_correct_responses.append(1) # 31 Iceland november\n",
    "mistral_correct_responses.append(1) # 32 flat Earth\n",
    "mistral_correct_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mistral_correct_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama2 responses\n",
    "We highlight this time some interesting outputs...\n",
    "\n",
    "It refused to answer the question about the hottest year:\n",
    "'I cannot predict the future or provide a definitive answer to your question. I can only provide information based on the data and models available to me.\\n'\n",
    "\n",
    "It gave a lot of precision in the confidence interval for some of the questions:\n",
    "\n",
    "'{\\n\"probability\": 0.8,\\n\"confidenceInterval\": [\"0.6953031763555094\", \"0.9046968236444906\"]\\n}\\n'\n",
    "\n",
    "'{\\n\"probability\": 0.000000456,\\n\"confidenceInterval\": [0.000000241, 0.000000718]\\n}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama2_correct_responses.append(1) # 18\n",
    "llama2_correct_responses.append(0) # 19\n",
    "llama2_correct_responses.append(0) # 20\n",
    "llama2_correct_responses.append(1) # 21 Artemis\n",
    "llama2_correct_responses.append(0) # 22 \n",
    "llama2_correct_responses.append(0) # 23\n",
    "llama2_correct_responses.append(0) # 24\n",
    "llama2_correct_responses.append(1) # 25\n",
    "llama2_correct_responses.append(0) # 26\n",
    "llama2_correct_responses.append(1) # 27\n",
    "llama2_correct_responses.append(1) # 28\n",
    "llama2_correct_responses.append(1) # 29\n",
    "llama2_correct_responses.append(0) # 30\n",
    "llama2_correct_responses.append(1) # 31\n",
    "llama2_correct_responses.append(1) # 32\n",
    "\n",
    "llama2_correct_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llama2_correct_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to have an idea how many right answers they got from very random outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46875"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mistral_correct_responses)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53125"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(llama2_correct_responses)/32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and takeaways\n",
    "The mistral model is clearly more advanced than the llama2 one. It is capable of giving more reasoning to the answers and of directing to reliable sources of information for those questions. Llama2 is just giving output probabilities and just luckily got more right answers than the mistral model. However this is not a reliable metric to compare both models.\n",
    "\n",
    "These models are not designed for these type of prediction tasks. In particular Mistral has been already warning us that the information is not to be trusted and we should use proper statistical algorithms and to collect real-time data to have better answers.\n",
    "\n",
    "Finally most of the answers rely on the 50% probability which is the same as saying \"I do not know\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "Using an API\n",
    "https://replicate.com/blog/run-mistral-7b-with-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
